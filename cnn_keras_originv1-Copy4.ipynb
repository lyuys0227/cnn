{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense, Dropout, Activation, Flatten  \n",
    "from keras.layers import Convolution2D, MaxPooling2D  \n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from scipy import misc\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import math\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델과 r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "width = 1\n",
    "height = 1\n",
    "\n",
    "# R^2\n",
    "def r_squared(y_true, y_hat):\n",
    "    ssr = 0\n",
    "    sst = 0\n",
    "    e = np.subtract(y_true, y_hat)\n",
    "    y_mean = np.mean(y_true)\n",
    "    for item in e:\n",
    "        ssr += item**2\n",
    "    for item in y_true:\n",
    "        sst += (item - y_mean)**2\n",
    "    r2 = 1 - ssr / sst\n",
    "    return r2\n",
    "\n",
    "\n",
    "def compile_model(model):\n",
    "    lrate = 0.01\n",
    "    sgd = SGD(lr=lrate, momentum=0.9, decay=1e-6, nesterov=True)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=sgd)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Convolution2D(32, 3, 3,\n",
    "                            border_mode='valid', \n",
    "                            input_shape=(100, 100, 3)))  \n",
    "    model.add(Activation('relu'))  \n",
    "    model.add(Convolution2D(32, 3, 3))  \n",
    "    model.add(Activation('relu'))  \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))  \n",
    "    model.add(Dropout(0.25))  \n",
    "      \n",
    "    model.add(Convolution2D(64, 3, 3, \n",
    "                            border_mode='valid'))  \n",
    "    model.add(Activation('relu'))  \n",
    "    model.add(Convolution2D(64, 3, 3))  \n",
    "    model.add(Activation('relu'))  \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))  \n",
    "    model.add(Dropout(0.25))  \n",
    "      \n",
    "    model.add(Flatten())  \n",
    "    model.add(Dense(256))  \n",
    "    model.add(Activation('relu'))  \n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(2))  \n",
    "    model.add(Activation('softmax'))  \n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이미지학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel_values():\n",
    "    file_name = './figures_v2'\n",
    "    pixels = []\n",
    "    for filename in glob.glob(file_name + '\\*.png'):\n",
    "        im = misc.imread(filename)\n",
    "        pixels.append(im)\n",
    "    return pixels\n",
    "    \n",
    "#이미지    \n",
    "def convert_image():\n",
    "    file_name = './figures_v2'\n",
    "    for filename in glob.glob(file_name + '\\*.png'):\n",
    "        img = Image.open(filename)\n",
    "        img = img.convert('RGB')\n",
    "        img.save(filename)\n",
    "    \n",
    "#이미지    \n",
    "def plot_data(data):\n",
    "    #t = np.arange(0, 29, 1) # 29 range맞게 수정 필요 \n",
    "    t = np.arange(0, 33, 1)\n",
    "    file_name_number = 0\n",
    "    fig = plt.figure(frameon=False, figsize=(width, height))\n",
    "    for group in data:\n",
    "        print(len(group))\n",
    "        #break\n",
    "        #count = 30 # 34개로 수정 시 수정 필요 \n",
    "        count = 34\n",
    "        #while count <= (len(group)-10)\n",
    "        while count <= (len(group)-5):\n",
    "            high = []\n",
    "            low = []\n",
    "            for item in group[count-34:count]:\n",
    "                high.append(item[0])\n",
    "                low.append(item[1])\n",
    "            file_name = r'\\fig_' + str(file_name_number)\n",
    "            ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "            ax.set_axis_off()\n",
    "            fig.add_axes(ax)\n",
    "            ax.plot(t, high[0:-1], 'b', t, low[0:-1], 'g')\n",
    "            fig.savefig('./figures_v2' + file_name, dpi=100)\n",
    "            fig.clf()\n",
    "            file_name_number += 1 # 이름이니까 해도되고 안해도 됨 \n",
    "            count += 10 #10개씩 만들고 싶다면 1개에서 10개로 수정 필요 \n",
    "            #count += 10\n",
    "    print('Created %d files!' % file_name_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수익률계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터추출 ---- 여기가 문제 같음 \n",
    "\n",
    "\n",
    "def extract_data():\n",
    "    file_name = 'data_3only1.csv' #해당파일 open\n",
    "    infile = open(file_name, 'r')\n",
    "    temp_buffer = []\n",
    "    for line in infile:\n",
    "        temp_buffer.append(line.strip('\\n')) ## 여기가 의심스러움 \n",
    "        #temp_buffer.append(line.strip(',')) ## 5개씩 엔터로 구분해야 한다. 근데 내 데이터는 그렇게 구성이 안되어있음 \n",
    "    #temp_buffer = temp_buffer[8:]\n",
    "    print('how many split?')\n",
    "    print(len(temp_buffer)) ##\n",
    "    temp_buffer = temp_buffer[8:] ### 수정\n",
    "    i = 0\n",
    "    groups = []\n",
    "    temp = []\n",
    "    for item in temp_buffer:\n",
    "        if i != 1223: #갯수 확인하기 1223개씩 나누기\n",
    "            temp.append(item)\n",
    "            i += 1 # n갯수 추가하기 2번 10개로 바꿨을 때 돌아가는지 확인\n",
    "        else:\n",
    "            groups.append(temp)\n",
    "            temp = []\n",
    "            i = 0\n",
    "    groups.append(temp)\n",
    "    infile.close() #해당파일 close \n",
    "    return groups\n",
    "#여기까지 csv파일 import시 영향 미치는 곳 \n",
    "#________________________________________________________________________________________________    \n",
    "#여기부터 가져온 파일 구분할 때 문제생기는 곳 \n",
    "\n",
    "\n",
    "#데이터분할 - 의심 2 ( 6개씩 나눠야 하는데, 시작이 하나씩 밀리는 이유가 여기? )\n",
    "def split_data(data):\n",
    "    groups = []\n",
    "    for item in data:\n",
    "        temp_buffer = []\n",
    "        for string in item:\n",
    "            number = string.split(',')\n",
    "            temp_buffer.append(number)\n",
    "        groups.append(temp_buffer)\n",
    "    #print(groups)# group 3개 0,1,2 \n",
    "    return groups\n",
    "\n",
    "\n",
    "#분할된 데이터 모으기\n",
    "def load_sample_data():\n",
    "    original_data = extract_data()\n",
    "    splitted_data = split_data(original_data)\n",
    "    useful_data = extract_useful_data(splitted_data)\n",
    "    #return useful_data, splitted_data\n",
    "    return useful_data #, original_data\n",
    "\n",
    "\n",
    "#필요정보취합\n",
    "def extract_useful_data(data):\n",
    "    groups = []\n",
    "    for group in data:\n",
    "        temp_buffer = []\n",
    "        for item in group:\n",
    "            temp = [item[2], item[3]]  # item - extract에서 등장\n",
    "            temp = [float(i) for i in temp]\n",
    "            temp_buffer.append(temp)\n",
    "            \n",
    "    #print(temp_buffer)\n",
    "    #print('aaaaa')## 확인\n",
    "        groups.append(temp_buffer)\n",
    "    return groups\n",
    "\n",
    "\n",
    "#수익률계산  \n",
    "def find_returns(data): \n",
    "    returns = []\n",
    "    price1 = []\n",
    "    price2 = []\n",
    "    for group in data:\n",
    "        # count = 34으로 고쳐야?\n",
    "        count = 34\n",
    "        while count <= (len(group)-5):\n",
    "            current_data = group[count-1]\n",
    "            future_data = group[count+4]\n",
    "            p1 = np.mean(current_data)\n",
    "            p2 = np.mean(future_data)\n",
    "            price1.append(p1) #\n",
    "            price2.append(p2) #\n",
    "            \n",
    "            \n",
    "            #수정함\n",
    "            #math.log(p2/p1)에서 \n",
    "            if p1 <= 0 or p2 <= 0: #S1. 아예 ()안이 값이 말이 안되는 경우 \n",
    "                returns.append(0)\n",
    "                count += 1\n",
    "            elif math.log(p2/p1)>= 2:\n",
    "                #S2. log(p2/p1)이 2보다 커서 값이 의미가 없는 경우                    \n",
    "                print(p1)\n",
    "                print(p2)\n",
    "                print('aaaa')\n",
    "                returns.append(0)\n",
    "                count += 1\n",
    "            else:    \n",
    "                returns.append(math.log(p2/p1)) #S3. 그외 정상적인경우\n",
    "                count += 1\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "#             if p1 <= 0 or p2 <= 0:\n",
    "#                 returns.append(0)\n",
    "#                 count += 1\n",
    "#             else:\n",
    "#                 if math.log(p2/p1)>=2:                    \n",
    "#                     print(p1)\n",
    "#                     print(p2)\n",
    "#                     print('aaaa')\n",
    "#                 #else:    \n",
    "#                 returns.append(math.log(p2/p1))\n",
    "#                 count += 1\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "#     print(returns)\n",
    "#     print(price1)\n",
    "#     print(price2)\n",
    "    return returns\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\tDATE\tCLOSE\tHIGH\tLOW\tOPEN\tVOLUME\n",
    "0\tEXCHANGE%3DNYSEARCA\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "1\tMARKET_OPEN_MINUTE=570\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "2\tMARKET_CLOSE_MINUTE=960\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "3\tINTERVAL=60\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "4\tCOLUMNS=DATE\tCLOSE\tHIGH\tLOW\tOPEN\tVOLUME\n",
    "5\tDATA=\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "6\tTIMEZONE_OFFSET=-240\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "7\ta1476451800\t214.19\t214.2\t214.14\t214.14\t185740 --- [7:]시 시작되는 기점 \n",
    "8\t1\t214.15\t214.22\t214.14\t214.19\t178844 -- [8:]시 시작되는 기점 \n",
    "\n",
    "\n",
    "* range[0,391,1] - 0 , 1~390까지 390개 6 리스트형으로 반복 나머지 0   \n",
    "  range[0, 1224] - 0, 1~1223까지 1223개 6리스트형으로 반복 나머지 5 \n",
    "   \n",
    "   ## 여기서 차이 나오는건가?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_sample_data\n",
      "how many split?\n",
      "3679\n",
      "1223\n",
      "1223\n",
      "1223\n",
      "Created 357 files!\n",
      "convert_image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\.conda\\envs\\ml\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1456\n",
      "3555\n",
      "WARNING:tensorflow:From C:\\Users\\user\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\user\\.conda\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\.conda\\envs\\ml\\lib\\site-packages\\ipykernel_launcher.py:32: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(100, 100,..., padding=\"valid\")`\n",
      "C:\\Users\\user\\.conda\\envs\\ml\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
      "C:\\Users\\user\\.conda\\envs\\ml\\lib\\site-packages\\ipykernel_launcher.py:40: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"valid\")`\n",
      "C:\\Users\\user\\.conda\\envs\\ml\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\.conda\\envs\\ml\\lib\\site-packages\\ipykernel_launcher.py:46: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 1456 input samples and 3555 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-662f14a1885a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-662f14a1885a>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m     model.fit(x_train, y_train, validation_data=(x_test, y_test), \n\u001b[0;32m     45\u001b[0m               \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m               shuffle=True, batch_size=100, verbose=1)\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;31m#    scores = model.evaluate(x_test, y_test, verbose=0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;31m#    print('Accuracy: %.2f%%' % (scores[1] * 100))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    802\u001b[0m             ]\n\u001b[0;32m    803\u001b[0m             \u001b[1;31m# Check that all arrays have the same length.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 804\u001b[1;33m             \u001b[0mcheck_array_length_consistency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    805\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m                 \u001b[1;31m# Additional checks to avoid users mistakenly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_array_length_consistency\u001b[1;34m(inputs, targets, weights)\u001b[0m\n\u001b[0;32m    235\u001b[0m                          \u001b[1;34m'the same number of samples as target arrays. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m                          \u001b[1;34m'Found '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' input samples '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[0;32m    238\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[1;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 1456 input samples and 3555 target samples."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 72x72 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    print('load_sample_data')\n",
    "    data= load_sample_data()\n",
    "    #data,x = load_sample_data() # x값확인위한 변수지정\n",
    "    #return 0\n",
    "    plot_data(data)\n",
    "    print('convert_image')\n",
    "    convert_image()\n",
    "    x = np.asarray(get_pixel_values())\n",
    "    y = np.asarray(find_returns(data))\n",
    "    x_train = x[0:len(x)] #1번 len(x),len(y)로 고쳤을 때 돌아가는지 확인 - checkO\n",
    "    y_train = y[0:len(y)]\n",
    "    x_test = x[0:len(x)]\n",
    "    y_test = y[0:len(y)]\n",
    "    print(len(x)) #\n",
    "    print(len(y)) #\n",
    "    \n",
    "#     print(len(x_train))\n",
    "#     print((y_train))\n",
    "#     print(len(x_test))\n",
    "#     print(len(y_test))\n",
    "    \n",
    "    #return 0 #2\n",
    "    #return\n",
    "#     x_train = x[0:4340] #1번 len(x)로 고쳤을 때 돌아가는지 확인\n",
    "#     y_train = y[0:4340]\n",
    "#     x_test = x[0:4340]\n",
    "#     y_test = y[0:4340]\n",
    "#    y_true = y_test\n",
    "#    y_train = np_utils.to_categorical(y_train, 2)\n",
    "#    y_test = np_utils.to_categorical(y_test, 2)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255.0\n",
    "    x_test /= 255.0\n",
    "    \n",
    "    #return 0 #1\n",
    "    model = create_model()\n",
    "    model = compile_model(model)\n",
    "\n",
    "    print('cnn')\n",
    "    # Fit the model\n",
    "    epochs = 1\n",
    "    model.fit(x_train, y_train, validation_data=(x_test, y_test), \n",
    "              nb_epoch=epochs,\n",
    "              shuffle=True, batch_size=100, verbose=1)\n",
    "#    scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "#    print('Accuracy: %.2f%%' % (scores[1] * 100))\n",
    "    classes = model.predict_classes(x_test, verbose=0)\n",
    "    classes = list(classes)\n",
    "    y_test = list(y_test)\n",
    "    r2 = r_squared(y_test, classes)\n",
    "    print(r2)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input arrays should have the same number of samples as target arrays. Found 357 input samples and 3556 target samples.\n",
    "\n",
    "len(x)357 , len(y)3556\n",
    "\n",
    "\n",
    "    print(len(x_test)) 357\n",
    "\n",
    "    print(len(y_test)) 3556\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,x = load_sample_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'x': x[0]},\n",
    "       \n",
    "        \n",
    "        \n",
    "#         'y':x[1][],\n",
    "#        'z':x[2][]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0, 33, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[0][1223]) \n",
    "print(x[1][1223])\n",
    "print(x[2][1223])\n",
    "\n",
    "#한칸이 밀려서 들어가서 1~ 1223,0 , 2~1, 3~2 이런식으로 들어가는것 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
